{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-hZjg7F2vWT"
      },
      "source": [
        "#Load the Data Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cK7BVpLovShp",
        "outputId": "dee75a0f-e61e-4988-c6f8-ef8714b3325d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Archive:  /content/drive/MyDrive/DL/CNN_Articels_clean.csv.zip\n",
            "  inflating: /content/CNN_Articels_clean.csv  \n",
            "First few rows of the dataset:\n",
            "   Index                                             Author  \\\n",
            "0      0                                 Jacopo Prisco, CNN   \n",
            "1      1                              Stephanie Bailey, CNN   \n",
            "2      2  Words by Stephanie Bailey, video by Zahra Jamshed   \n",
            "3      3                                     Kathryn Vasel    \n",
            "4      4                    Paul R. La Monica, CNN Business   \n",
            "\n",
            "        Date published  Category    Section  \\\n",
            "0  2021-07-15 02:46:59      news      world   \n",
            "1  2021-05-12 07:52:09      news      world   \n",
            "2  2021-06-16 02:51:30      news       asia   \n",
            "3  2022-03-18 14:37:21  business    success   \n",
            "4  2022-03-19 11:41:08  business  investing   \n",
            "\n",
            "                                                 Url  \\\n",
            "0  https://www.cnn.com/2021/07/14/world/tusimple-...   \n",
            "1  https://www.cnn.com/2021/05/12/world/ironhand-...   \n",
            "2  https://www.cnn.com/2021/06/15/asia/swarm-robo...   \n",
            "3  https://www.cnn.com/2022/03/18/success/pandemi...   \n",
            "4  https://www.cnn.com/2022/03/19/investing/march...   \n",
            "\n",
            "                                            Headline  \\\n",
            "0  There's a shortage of truckers, but TuSimple t...   \n",
            "1  Bioservo's robotic 'Ironhand' could protect fa...   \n",
            "2  This swarm of robots gets smarter the more it ...   \n",
            "3  Two years later, remote work has changed milli...   \n",
            "4          Why March is so volatile for stocks - CNN   \n",
            "\n",
            "                                         Description  \\\n",
            "0  The e-commerce boom has exacerbated a global t...   \n",
            "1  Working in a factory can mean doing the same t...   \n",
            "2  In a Hong Kong warehouse, a swarm of autonomou...   \n",
            "3  Here's a look at how the pandemic reshaped peo...   \n",
            "4  March Madness isn't just for college basketbal...   \n",
            "\n",
            "                                            Keywords  \\\n",
            "0  world, There's a shortage of truckers, but TuS...   \n",
            "1  world, Bioservo's robotic 'Ironhand' could pro...   \n",
            "2  asia, This swarm of robots gets smarter the mo...   \n",
            "3  success, Two years later, remote work has chan...   \n",
            "4  investing, Why March is so volatile for stocks...   \n",
            "\n",
            "                                     Second headline  \\\n",
            "0  There's a shortage of truckers, but TuSimple t...   \n",
            "1  A robotic 'Ironhand' could protect factory wor...   \n",
            "2  This swarm of robots gets smarter the more it ...   \n",
            "3  Two years later, remote work has changed milli...   \n",
            "4                Why March is so volatile for stocks   \n",
            "\n",
            "                                        Article text  \n",
            "0   (CNN)Right now, there's a shortage of truck d...  \n",
            "1   (CNN)Working in a factory or warehouse can me...  \n",
            "2   (CNN)In a Hong Kong warehouse, a swarm of aut...  \n",
            "3  The pandemic thrust the working world into a n...  \n",
            "4  New York (CNN Business)March Madness isn't jus...  \n",
            "\n",
            "Missing values in each column:\n",
            "Index              0\n",
            "Author             0\n",
            "Date published     0\n",
            "Category           0\n",
            "Section            0\n",
            "Url                0\n",
            "Headline           0\n",
            "Description        0\n",
            "Keywords           0\n",
            "Second headline    0\n",
            "Article text       9\n",
            "dtype: int64\n",
            "\n",
            "Data Summary:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 37949 entries, 0 to 37948\n",
            "Data columns (total 11 columns):\n",
            " #   Column           Non-Null Count  Dtype \n",
            "---  ------           --------------  ----- \n",
            " 0   Index            37949 non-null  int64 \n",
            " 1   Author           37949 non-null  object\n",
            " 2   Date published   37949 non-null  object\n",
            " 3   Category         37949 non-null  object\n",
            " 4   Section          37949 non-null  object\n",
            " 5   Url              37949 non-null  object\n",
            " 6   Headline         37949 non-null  object\n",
            " 7   Description      37949 non-null  object\n",
            " 8   Keywords         37949 non-null  object\n",
            " 9   Second headline  37949 non-null  object\n",
            " 10  Article text     37940 non-null  object\n",
            "dtypes: int64(1), object(10)\n",
            "memory usage: 3.2+ MB\n",
            "None\n",
            "\n",
            "Distribution of categories:\n",
            "Category\n",
            "news             18077\n",
            "sport            15542\n",
            "politics          2461\n",
            "business           854\n",
            "health             557\n",
            "entertainment      413\n",
            "travel              39\n",
            "vr                   5\n",
            "style                1\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "# Unzip the file\n",
        "!unzip '/content/drive/MyDrive/DL/CNN_Articels_clean.csv.zip' -d '/content/'\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/CNN_Articels_clean.csv')\n",
        "\n",
        "# Check the first few rows of the dataset\n",
        "print(\"First few rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing values in each column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Get a summary of the data types and non-null counts\n",
        "print(\"\\nData Summary:\")\n",
        "print(df.info())\n",
        "\n",
        "# Check the distribution of categories\n",
        "print(\"\\nDistribution of categories:\")\n",
        "print(df['Category'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCcwoky43BZ6"
      },
      "source": [
        "#Clean Data Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YECYds3NvYAq",
        "outputId": "afced100-06a3-4560-d58b-900f815e96b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Missing values after cleaning:\n",
            "Index              0\n",
            "Author             0\n",
            "Date published     0\n",
            "Category           0\n",
            "Section            0\n",
            "Url                0\n",
            "Headline           0\n",
            "Description        0\n",
            "Keywords           0\n",
            "Second headline    0\n",
            "Article text       0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Drop rows with missing 'Article text'\n",
        "df_cleaned = df.dropna(subset=['Article text'])\n",
        "\n",
        "# Verify there are no more missing values in the 'Article text' column\n",
        "print(\"\\nMissing values after cleaning:\")\n",
        "print(df_cleaned.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NAZ95aIvaaB",
        "outputId": "45a51753-e488-4b82-da07-579feb37e17b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    Right now, there's a shortage of truck drivers...\n",
            "1    Working in a factory or warehouse can mean doi...\n",
            "2    In a Hong Kong warehouse, a swarm of autonomou...\n",
            "3    The pandemic thrust the working world into a n...\n",
            "4    New York (CNN Business)March Madness isn't jus...\n",
            "Name: Article text, dtype: object\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-d6b8de0340dc>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_cleaned['Article text'] = df_cleaned['Article text'].apply(remove_cnn_phrase)\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Function to remove the phrase '(CNN)' from the article text\n",
        "def remove_cnn_phrase(text):\n",
        "    # Remove variations of (CNN), (cnn), or extra spaces\n",
        "    return re.sub(r'\\(cnn\\)|\\(CNN\\)', '', text).strip()\n",
        "\n",
        "# Apply the function to the 'Article text' column\n",
        "df_cleaned['Article text'] = df_cleaned['Article text'].apply(remove_cnn_phrase)\n",
        "\n",
        "# Check if '(CNN)' was successfully removed\n",
        "print(df_cleaned['Article text'].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_iUV81WveYB",
        "outputId": "f0ffd05b-5df4-4c57-f6e5-8608cf0dc2aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Invalid dates after conversion:\n",
            "Empty DataFrame\n",
            "Columns: [Index, Author, Date published, Category, Section, Url, Headline, Description, Keywords, Second headline, Article text]\n",
            "Index: []\n",
            "\n",
            "Final dataset shape after cleaning:\n",
            "(37940, 11)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-4-c97cd517248b>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_cleaned['Date published'] = pd.to_datetime(df_cleaned['Date published'], errors='coerce')\n"
          ]
        }
      ],
      "source": [
        "# Convert 'Date published' to datetime format\n",
        "df_cleaned['Date published'] = pd.to_datetime(df_cleaned['Date published'], errors='coerce')\n",
        "\n",
        "# Check for any invalid dates that couldn't be converted\n",
        "print(\"\\nInvalid dates after conversion:\")\n",
        "print(df_cleaned[df_cleaned['Date published'].isnull()])\n",
        "\n",
        "# Drop rows with invalid dates, if necessary\n",
        "df_cleaned = df_cleaned.dropna(subset=['Date published'])\n",
        "\n",
        "# Check final shape of the cleaned dataset\n",
        "print(\"\\nFinal dataset shape after cleaning:\")\n",
        "print(df_cleaned.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLOdxkLmvhGY"
      },
      "outputs": [],
      "source": [
        "# Save the cleaned dataset to a new CSV file\n",
        "df_cleaned.to_csv('data/cnn_articles_cleaned.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IDkX-H53NYr"
      },
      "source": [
        "#Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmI4In0_3UyD",
        "outputId": "3338b85d-2ad4-431f-e7d6-8e5ac250e304"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                        Article text  \\\n",
            "0  Right now, there's a shortage of truck drivers...   \n",
            "1  Working in a factory or warehouse can mean doi...   \n",
            "2  In a Hong Kong warehouse, a swarm of autonomou...   \n",
            "3  The pandemic thrust the working world into a n...   \n",
            "4  New York (CNN Business)March Madness isn't jus...   \n",
            "\n",
            "                                        cleaned_text  \n",
            "0  right theres shortage truck drivers us worldwi...  \n",
            "1  working factory warehouse mean task repetition...  \n",
            "2  hong kong warehouse swarm autonomous robots wo...  \n",
            "3  pandemic thrust working world new reality marc...  \n",
            "4  new york cnn businessmarch madness isnt colleg...  \n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download stopwords from NLTK\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Define stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function for text preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation and special characters\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Remove stop words\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Join the words back into a single string\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply preprocessing to the 'Article text' column\n",
        "df_cleaned['cleaned_text'] = df_cleaned['Article text'].apply(preprocess_text)\n",
        "\n",
        "# Check the first few rows of the cleaned text\n",
        "print(df_cleaned[['Article text', 'cleaned_text']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEAvrBGa3Xha"
      },
      "source": [
        "#Tokenization and Padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMro_SX93gnq",
        "outputId": "ebeb65b9-336b-4bfc-a574-55aaf6837298"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X (features): (37940, 100)\n",
            "Shape of y (labels): (37940,)\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the maximum number of words to use (top 20,000 most frequent words)\n",
        "max_words = 20000\n",
        "# Define the maximum length of a sequence (for padding)\n",
        "max_len = 100\n",
        "\n",
        "# Initialize the tokenizer and fit on the cleaned text\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(df_cleaned['cleaned_text'])\n",
        "\n",
        "# Convert text to sequences of integers\n",
        "sequences = tokenizer.texts_to_sequences(df_cleaned['cleaned_text'])\n",
        "\n",
        "# Apply padding to ensure all sequences have the same length\n",
        "X = pad_sequences(sequences, maxlen=max_len)\n",
        "\n",
        "# Convert categories to integer labels (for classification)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Initialize the label encoder\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(df_cleaned['Category'])\n",
        "\n",
        "#New\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print shapes of the final processed data\n",
        "print(\"Shape of X (features):\", X.shape)\n",
        "print(\"Shape of y (labels):\", y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIo_-d-q3fgS"
      },
      "source": [
        "#Build LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lcau3ooh-hFs",
        "outputId": "d4036f2f-327d-493e-f57c-55de52d25334"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of unique categories: 9\n"
          ]
        }
      ],
      "source": [
        "#Number of unique news categories.\n",
        "\n",
        "unique_category_count = df['Category'].nunique()\n",
        "print(f\"Number of unique categories: {unique_category_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 879
        },
        "id": "pIhRJB6v3tPC",
        "outputId": "3feccccb-89b3-4c76-958e-77a340bbfb81"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">585</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m2,560,000\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m131,584\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m49,408\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)                   │             \u001b[38;5;34m585\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,742,345</span> (10.46 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,742,345\u001b[0m (10.46 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,741,961</span> (10.46 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,741,961\u001b[0m (10.46 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> (1.50 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m384\u001b[0m (1.50 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 188ms/step - accuracy: 0.7065 - loss: 2.7677 - val_accuracy: 0.7671 - val_loss: 1.1396 - learning_rate: 0.0010\n",
            "Epoch 2/20\n",
            "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 182ms/step - accuracy: 0.9013 - loss: 0.5787 - val_accuracy: 0.8824 - val_loss: 0.5245 - learning_rate: 0.0010\n",
            "Epoch 3/20\n",
            "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 189ms/step - accuracy: 0.9252 - loss: 0.3618 - val_accuracy: 0.8702 - val_loss: 0.5339 - learning_rate: 0.0010\n",
            "Epoch 4/20\n",
            "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 181ms/step - accuracy: 0.9380 - loss: 0.2926 - val_accuracy: 0.8778 - val_loss: 0.5007 - learning_rate: 0.0010\n",
            "Epoch 5/20\n",
            "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 194ms/step - accuracy: 0.9418 - loss: 0.2654 - val_accuracy: 0.8908 - val_loss: 0.5122 - learning_rate: 0.0010\n",
            "Epoch 6/20\n",
            "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 187ms/step - accuracy: 0.9483 - loss: 0.2329 - val_accuracy: 0.8542 - val_loss: 0.5982 - learning_rate: 0.0010\n",
            "Epoch 7/20\n",
            "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 191ms/step - accuracy: 0.9593 - loss: 0.1760 - val_accuracy: 0.8888 - val_loss: 0.4769 - learning_rate: 5.0000e-04\n",
            "Epoch 8/20\n",
            "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 185ms/step - accuracy: 0.9684 - loss: 0.1420 - val_accuracy: 0.8831 - val_loss: 0.5356 - learning_rate: 5.0000e-04\n",
            "Epoch 9/20\n",
            "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 190ms/step - accuracy: 0.9707 - loss: 0.1285 - val_accuracy: 0.8829 - val_loss: 0.5796 - learning_rate: 5.0000e-04\n",
            "Epoch 10/20\n",
            "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 186ms/step - accuracy: 0.9708 - loss: 0.1302 - val_accuracy: 0.8747 - val_loss: 0.6327 - learning_rate: 5.0000e-04\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dropout, BatchNormalization, Dense\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Define the LSTM model\n",
        "model = Sequential()\n",
        "\n",
        "# Embedding layer\n",
        "model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_len, input_shape=(max_len,)))\n",
        "\n",
        "# LSTM layer with L2 regularization and Dropout\n",
        "model.add(LSTM(128, activation='relu', kernel_regularizer=l2(0.01), return_sequences=True))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Batch normalization\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# LSTM layer with L2 regularization\n",
        "model.add(LSTM(64, activation='tanh', kernel_regularizer=l2(0.01)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Batch normalization\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Output layer with L2 regularization\n",
        "model.add(Dense(unique_category_count, activation='softmax', kernel_regularizer=l2(0.01)))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0005)\n",
        "\n",
        "#model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "# history = model.fit(X, y, epochs=20, batch_size=64, validation_split=0.2, callbacks=[early_stopping, reduce_lr])\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=64, validation_split=0.2, callbacks=[early_stopping, reduce_lr])\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
